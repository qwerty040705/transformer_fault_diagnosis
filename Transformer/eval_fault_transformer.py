# Transformer/eval_fault_transformer.py
import os
import numpy as np
import torch
from torch.utils.data import TensorDataset, DataLoader, random_split
from sklearn.metrics import roc_auc_score, average_precision_score, f1_score
from fault_diagnosis_model import FaultDiagnosisTransformer
import matplotlib.pyplot as plt

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
repo_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
device     = torch.device("cuda" if torch.cuda.is_available() else "cpu")
batch_size = 64
seed       = 42
print("device:", device)

# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
link_count = int(input("How many links?: ") or 1)
data_path  = os.path.join(repo_root, f"data_storage/link_{link_count}/fault_dataset.npz")
ckpt_path  = os.path.join(repo_root, "Transformer", f"Transformer_link_{link_count}.pth")

# â”€â”€ Load data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
data    = np.load(data_path)
desired = data["desired"]                      # (S,T,4,4)
actual  = data["actual"]
labels  = data["label"].astype(np.float32)     # (S,T,M)  # 1=ì •ìƒ, 0=ê³ ì¥
dt      = float(data.get("dt", 0.01))
frame_hz= 1.0 / dt

S, T, _, _ = desired.shape
M = labels.shape[2]

des_12 = desired[:, :, :3, :4].reshape(S, T, 12)
act_12 = actual[:,  :, :3, :4].reshape(S, T, 12)
X  = np.concatenate([des_12, act_12], axis=2).astype(np.float32)   # (S,T,24)
y  = labels                                                         # (S,T,M)

# â”€â”€ Load checkpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    ckpt = torch.load(ckpt_path, map_location=device, weights_only=True)
except Exception:
    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)

mean, std  = ckpt["train_mean"], ckpt["train_std"]
if isinstance(mean, torch.Tensor): mean = mean.cpu().numpy()
if isinstance(std, torch.Tensor):  std  = std.cpu().numpy()
cfg        = ckpt["cfg"]
assert (ckpt["input_dim"], ckpt["T"], ckpt["M"]) == (24, T, M), "shape mismatch"

# â”€â”€ Normalize & val split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
X = (X - mean) / std
ds_all = TensorDataset(torch.from_numpy(X), torch.from_numpy(y))
train_sz = int(0.8 * S); val_sz = S - train_sz
_, val_ds = random_split(ds_all, [train_sz, val_sz],
                         generator=torch.Generator().manual_seed(seed))
val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)

# â”€â”€ Rebuild model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model = FaultDiagnosisTransformer(
    input_dim=24,
    d_model=cfg["d_model"], nhead=cfg["nhead"],
    num_layers=cfg["num_layers"], dim_feedforward=cfg["dim_feedforward"],
    dropout=cfg["dropout"], output_dim=M, max_seq_len=T
).to(device)
model.load_state_dict(ckpt["model_state"])
model.eval()

# â”€â”€ Inference â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_prob, all_pred, all_true = [], [], []
sigmoid = torch.nn.Sigmoid()
with torch.no_grad():
    for xb, yb in val_loader:
        logits = model(xb.to(device))          # (B,T,M)
        prob   = sigmoid(logits).cpu()         # ì˜ˆì¸¡í™•ë¥ (p=ì •ìƒì¼ í™•ë¥ )
        pred   = (prob >= 0.5).int()           # 1=ì •ìƒ, 0=ê³ ì¥
        all_prob.append(prob)
        all_pred.append(pred)
        all_true.append(yb.int())

prob = torch.cat(all_prob, 0)   # (N,T,M)  p(normal)
pred = torch.cat(all_pred, 0)   # (N,T,M)  1=normal,0=fault
true = torch.cat(all_true, 0)   # (N,T,M)  1=normal,0=fault
N = true.shape[0]

# ======================== ìœ í‹¸ =========================
def first_zero_idx(seq_0or1: np.ndarray, min_run: int = 1):
    """ê°’==0(ê³ ì¥)ì´ min_run í”„ë ˆì„ ì´ìƒ ì—°ì†ìœ¼ë¡œ ì²˜ìŒ ë“±ì¥í•˜ëŠ” ì‹œì . ì—†ìœ¼ë©´ None."""
    if min_run <= 1:
        return int(np.argmax(seq_0or1 == 0)) if (seq_0or1 == 0).any() else None
    run = 0
    for t, v in enumerate(seq_0or1):
        run = run + 1 if v == 0 else 0
        if run >= min_run:
            return t - min_run + 1
    return None

def run_filter_zero_mask(seq_0or1: np.ndarray, min_run: int = 1) -> np.ndarray:
    """0(ê³ ì¥)ì´ ì—°ì† min_run ì´ìƒì¸ êµ¬ê°„ë§Œ Trueë¡œ ì¸ì •í•˜ëŠ” ë§ˆìŠ¤í¬."""
    if min_run <= 1:
        return (seq_0or1 == 0)
    T = len(seq_0or1)
    mask = np.zeros(T, dtype=bool)
    run = 0
    for t, v in enumerate(seq_0or1):
        run = run + 1 if v == 0 else 0
        if run >= min_run:
            mask[t - min_run + 1 : t + 1] = True
    return mask

def fault_set(mat_TxM: np.ndarray, min_run: int = 1):
    """(T,M)ì—ì„œ 0(ê³ ì¥)ì¸ ëª¨í„° ID ì§‘í•© ë°˜í™˜."""
    T, M = mat_TxM.shape
    s = set()
    for m in range(M):
        if first_zero_idx(mat_TxM[:, m], min_run=min_run) is not None:
            s.add(m)
    return s

# ============ Top-K íŒ¨í„´ ë§¤ì¹­(ë‹¤ìˆ˜ íŒ¨í„´) ìœ í‹¸ =============
def hamming(a: np.ndarray, b: np.ndarray) -> int:
    return int(np.sum(a != b))

def top_k_patterns_with_tol(mat_TxM: np.ndarray, k=2, tol=0):
    """
    í–‰ íŒ¨í„´ì„ tol í•´ë° ë°˜ê²½ìœ¼ë¡œ ê°„ì´ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ ì¹´ìš´íŠ¸.
    ë°˜í™˜: (patterns[list[np.ndarray]], counts[list[int]], coverage[0..1])
    """
    T, M = mat_TxM.shape
    protos = []   # ë¦¬ìŠ¤íŠ¸[(íŒ¨í„´(np.array), count)]
    for t in range(T):
        row = mat_TxM[t]
        assigned = False
        for idx, (p, c) in enumerate(protos):
            if hamming(row, p) <= tol:
                protos[idx] = (p, c+1)
                assigned = True
                break
        if not assigned:
            protos.append((row.copy(), 1))
    protos_sorted = sorted(protos, key=lambda pc: pc[1], reverse=True)
    top = protos_sorted[:k]
    total = float(T)
    patterns = [p for p, _ in top]
    counts   = [c for _, c in top]
    coverage = sum(counts) / total if total > 0 else 0.0
    return patterns, counts, coverage

def equal_pattern_multiset(pats_a, pats_b):
    """top-k íŒ¨í„´ë“¤ì„ ë©€í‹°ì…‹ ë¹„êµ(ìˆœì„œ ë¬´ì‹œ, ì¤‘ë³µ í—ˆìš©)."""
    if len(pats_a) != len(pats_b):
        return False
    aa = [tuple(x.tolist()) for x in pats_a]
    bb = [tuple(x.tolist()) for x in pats_b]
    from collections import Counter
    return Counter(aa) == Counter(bb)

def first_index_close(mat_TxM: np.ndarray, pattern: np.ndarray, tol: int = 0):
    """í•´ë° ê±°ë¦¬ tol ì´ë‚´ë¡œ ì¼ì¹˜í•˜ëŠ” ì²« í”„ë ˆì„ ì¸ë±ìŠ¤ (ì—†ìœ¼ë©´ None)."""
    for t in range(mat_TxM.shape[0]):
        if hamming(mat_TxM[t], pattern) <= tol:
            return t
    return None

# ======================================================
# A) ë§ˆì´í¬ë¡œ ì§€í‘œ(AUROC/AUPRC/F1) â€” ì–‘ì„±=ê³ ì¥ìœ¼ë¡œ ê³„ì‚°
prob_fault = (1.0 - prob).view(-1).numpy()          # ì–‘ì„±=ê³ ì¥ í™•ë¥ 
true_fault = (1 - true).view(-1).numpy()            # 1=ê³ ì¥, 0=ì •ìƒ
pred_fault = (1 - pred).view(-1).numpy()            # 1=ê³ ì¥, 0=ì •ìƒ

try:
    auroc_micro = roc_auc_score(true_fault, prob_fault)
except ValueError:
    auroc_micro = np.nan
auprc_micro = average_precision_score(true_fault, prob_fault)
f1_micro    = f1_score(true_fault, pred_fault, average="micro", zero_division=0)

print("\n==== Micro metrics (positive = FAULT) ====")
print(f"AUROC  : {auroc_micro:.4f}")
print(f"AUPRC  : {auprc_micro:.4f}")
print(f"F1@0.5 : {f1_micro:.4f}")

# ======================================================
# B) ì´ë²¤íŠ¸ ì§€í‘œ(ëª¨í„° ë‹¨ìœ„ TP/FP/FN) + Delay(ì„±ê³µ íƒì§€ë§Œ)
tp = fp = fn = 0
detected_delays = []  # seconds
for i in range(N):
    gt = true[i].numpy()   # 1=ì •ìƒ,0=ê³ ì¥
    pr = pred[i].numpy()
    for m in range(M):
        gt_seq = gt[:, m]; pr_seq = pr[:, m]
        if 0 in gt_seq:                      # ì‹¤ì œ ê³ ì¥
            t_true = int(np.argmax(gt_seq == 0))
            if 0 in pr_seq:                  # í•´ë‹¹ ëª¨í„° íƒì§€
                t_pred = int(np.argmax(pr_seq == 0))
                tp += 1
                detected_delays.append(max(t_pred - t_true, 0) * dt)
            else:
                fn += 1
        else:
            if 0 in pr_seq:
                fp += 1

precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0
f1_event  = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0.0

print("\n==== [Event-level] motor-wise detection ====")
print(f"TP events: {tp} / {tp + fn}")
print(f"Precision={precision:.4f}  Recall={recall:.4f}  F1={f1_event:.4f}")

if detected_delays:
    d = np.array(detected_delays)
    print(f"Delay (TP only) â†’ mean={d.mean():.4f}s, median={np.median(d):.4f}s, n={len(d)}")
else:
    print("Delay: (no TP)")

# ======================================================
# C) Strict ìƒ˜í”Œ ì •í™•ë„ (ëª¨í„° ID ì§‘í•© ì™„ì „ ì¼ì¹˜, ì‹œê°„ ë¬´ì‹œ)
MIN_RUN_PRED = 1  # ì˜ˆì¸¡ ìŠ¤íŒŒì´í¬ ì–µì œ ì›í•˜ë©´ 2~3ìœ¼ë¡œ
strict_ok = 0
for i in range(N):
    gt = true[i].numpy(); pr = pred[i].numpy()
    gt_set = fault_set(gt, min_run=1)
    pr_set = fault_set(pr, min_run=MIN_RUN_PRED)
    if gt_set == pr_set:
        strict_ok += 1
print("\n==== [Strict] sample accuracy (ID set equality) ====")
print(f"Strict acc: {strict_ok}/{N} = {strict_ok/max(N,1):.4f}")

# ======================================================
# D) Lenient ìƒ˜í”Œ ì •í™•ë„ (ê³ ì¥ êµ¬ê°„ 90% ì´ìƒ ë§ì¶”ë©´ ì„±ê³µ) + Delay(ì„±ê³µë§Œ)
TAU_RECALL   = 0.90   # 90% ê·œì¹™
MAX_FP_RATE  = 0.10   # ì •ìƒ í”„ë ˆì„ FPìœ¨ ì œí•œ(ë„ë ¤ë©´ None)
MIN_RUN_PRED = 1      # ì˜ˆì¸¡ ìŠ¤íŒŒì´í¬ ì–µì œ

lenient_ok = 0
lenient_delays = []

for i in range(N):
    gt = true[i].numpy(); pr = pred[i].numpy()
    gt_fault_motors = [m for m in range(M) if (gt[:, m] == 0).any()]
    if not gt_fault_motors:
        continue

    # ëª¨í„°ë³„ ì‹œê°„ ê¸°ë°˜ recall
    recalls = []
    per_motor_delays = []
    for m in gt_fault_motors:
        gt_fault_mask = (gt[:, m] == 0)
        pr_fault_mask = run_filter_zero_mask(pr[:, m], MIN_RUN_PRED)
        denom = gt_fault_mask.sum()
        if denom == 0:
            continue
        recalls.append(((gt_fault_mask & pr_fault_mask).sum()) / denom)

        t_true = first_zero_idx(gt[:, m], 1)
        t_pred = first_zero_idx(pr[:, m], MIN_RUN_PRED)
        if t_true is not None and t_pred is not None:
            per_motor_delays.append(max(t_pred - t_true, 0) * dt)

    if not recalls:
        continue
    mean_rec = float(np.mean(recalls))

    fp_ok = True
    if MAX_FP_RATE is not None:
        gt_normal_mask = (gt == 1)
        pr_fault_mask_all = np.stack(
            [run_filter_zero_mask(pr[:, m], MIN_RUN_PRED) for m in range(M)], axis=1
        )
        fp_num = (pr_fault_mask_all & gt_normal_mask).sum()
        fp_den = gt_normal_mask.sum()
        fp_rate = fp_num / max(fp_den, 1)
        fp_ok = (fp_rate <= MAX_FP_RATE)

    if (mean_rec >= TAU_RECALL) and fp_ok:
        lenient_ok += 1
        lenient_delays.extend(per_motor_delays)

print("\n==== [Lenient] sample accuracy (â‰¥90% time coverage) ====")
print(f"Lenient acc: {lenient_ok}/{N} = {lenient_ok/max(N,1):.4f}")
if lenient_delays:
    d = np.array(lenient_delays)
    print(f"Lenient delay (success only) â†’ mean={d.mean():.4f}s, median={np.median(d):.4f}s, n={len(lenient_delays)}")
else:
    print("Lenient delay: (no lenient successes)")

# ======================================================
# E) Delay ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ (TP ê¸°ì¤€) â”€ 0~0.2s êµ¬ê°„ ë¹„ìœ¨
if detected_delays:
    d = np.array(detected_delays)
    counts, bins = np.histogram(d, bins=50, range=(0.0, 0.2))
    pct = counts / len(d) * 100.0

    plt.figure(figsize=(6,4))
    plt.bar(bins[:-1], pct, width=(bins[1]-bins[0]), edgecolor='black', align='edge')
    plt.axvline(0.05, linestyle='--', label='0.05s')
    plt.axvline(0.10, linestyle='--', label='0.10s')
    plt.axvline(0.20, linestyle='--', label='0.20s')
    plt.xlabel('Detection Delay (s)'); plt.ylabel('Percentage of Cases (%)')
    plt.title('Detection Delay Distribution (TP only, 0â€“0.2s)')
    plt.legend(); plt.grid(True, linestyle='--', alpha=0.6); plt.tight_layout()
    save_path_pct = os.path.join(repo_root, "delay_hist_percentage.png")
    plt.savefig(save_path_pct, dpi=300)
    print(f"\nğŸ“ Percentage histogram saved to: {save_path_pct}")

    within_005 = np.mean(d <= 0.05); within_010 = np.mean(d <= 0.10); within_020 = np.mean(d <= 0.20)
    print(f"â± Delay â‰¤ 0.05s : {within_005*100:.2f}%")
    print(f"â± Delay â‰¤ 0.10s : {within_010*100:.2f}%")
    print(f"â± Delay â‰¤ 0.20s : {within_020*100:.2f}%")
else:
    print("\n(No TP delays to plot)")

# ======================================================
# F) Majority Top-2 íŒ¨í„´ ë§¤ì¹­ ì •í™•ë„ + Delay(ì„±ê³µë§Œ)
TOP_K = 2
HAMMING_TOL = 0     # íŒ¨í„´ í—ˆìš© í•´ë° ê±°ë¦¬(ì‘ì€ ì˜¤ë¥˜ í¡ìˆ˜). 0~2 ê¶Œì¥
COVERAGE_TAU = 0.8  # top-2 íŒ¨í„´ì´ ì „ì²´ í”„ë ˆì„ì˜ 80% ì´ìƒì„ ë®ì–´ì•¼ ìœ íš¨

def hamming(a: np.ndarray, b: np.ndarray) -> int:
    return int(np.sum(a != b))

def top_k_patterns_with_tol(mat_TxM: np.ndarray, k=2, tol=0):
    """
    í–‰ íŒ¨í„´ì„ tol í•´ë° ë°˜ê²½ìœ¼ë¡œ ê°„ì´ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ ì¹´ìš´íŠ¸.
    ë°˜í™˜: (patterns[list[np.ndarray]], counts[list[int]], coverage[0..1])
    """
    T, M = mat_TxM.shape
    protos = []   # ë¦¬ìŠ¤íŠ¸[(íŒ¨í„´(np.array), count)]
    for t in range(T):
        row = mat_TxM[t]
        assigned = False
        for idx, (p, c) in enumerate(protos):
            if hamming(row, p) <= tol:
                protos[idx] = (p, c+1)
                assigned = True
                break
        if not assigned:
            protos.append((row.copy(), 1))
    protos_sorted = sorted(protos, key=lambda pc: pc[1], reverse=True)
    top = protos_sorted[:k]
    total = float(T)
    patterns = [p for p, _ in top]
    counts   = [c for _, c in top]
    coverage = (sum(counts) / total) if total > 0 else 0.0
    return patterns, counts, coverage

def can_match_topk(pats_a, pats_b, tol=0):
    """
    pats_a, pats_b: list[np.ndarray] (ê¸¸ì´ ë™ì¼)
    í•´ë° ê±°ë¦¬ â‰¤ tol ì¡°ê±´ìœ¼ë¡œ 1:1 ë§¤ì¹­ì´ ëª¨ë‘ ì„±ë¦½í•˜ë©´ True.
    (ìˆœì„œ ë¬´ì‹œ, ì¹´ìš´íŠ¸ëŠ” ì´ë¯¸ top-kì— ë°˜ì˜ë˜ì—ˆë‹¤ê³  ë³´ê³  íŒ¨í„´ ë‚´ìš©ë§Œ ë§¤ì¹­)
    """
    if len(pats_a) != len(pats_b):
        return False
    used = [False] * len(pats_b)
    for a in pats_a:
        found = False
        for j, b in enumerate(pats_b):
            if not used[j] and hamming(a, b) <= tol:
                used[j] = True
                found = True
                break
        if not found:
            return False
    return True

def first_index_close(mat_TxM: np.ndarray, pattern: np.ndarray, tol: int = 0):
    """í•´ë° ê±°ë¦¬ tol ì´ë‚´ë¡œ ì¼ì¹˜í•˜ëŠ” ì²« í”„ë ˆì„ ì¸ë±ìŠ¤ (ì—†ìœ¼ë©´ None)."""
    for t in range(mat_TxM.shape[0]):
        if hamming(mat_TxM[t], pattern) <= tol:
            return t
    return None

maj_ok = 0
maj_delays = []  # seconds

for i in range(N):
    gt = true[i].numpy()   # (T,M)
    pr = pred[i].numpy()

    gt_pats, gt_cnts, gt_cov = top_k_patterns_with_tol(gt, k=TOP_K, tol=HAMMING_TOL)
    pr_pats, pr_cnts, pr_cov = top_k_patterns_with_tol(pr, k=TOP_K, tol=HAMMING_TOL)

    # ì»¤ë²„ë¦¬ì§€ ë‚®ìœ¼ë©´ ë…¸ì´ì¦ˆê°€ ë§ë‹¤ê³  ë³´ê³  ì‹¤íŒ¨ ì²˜ë¦¬(ì›í•˜ë©´ ì™„í™” ê°€ëŠ¥)
    if (gt_cov < COVERAGE_TAU) or (pr_cov < COVERAGE_TAU):
        continue

    # ìµœë‹¤ top-k íŒ¨í„´ì´ tol ì´ë‚´ì—ì„œ 1:1 ë§¤ì¹­ë˜ë©´ ì •ë‹µ
    if can_match_topk(gt_pats, pr_pats, tol=HAMMING_TOL):
        maj_ok += 1

        # Delay: "ê³ ì¥ íŒ¨í„´"(í–‰ì— 0ì´ í•˜ë‚˜ë¼ë„ ìˆëŠ” íŒ¨í„´)ì„ ì°¾ì•„ ì²« ë“±ì¥ ì‹œì  ì°¨ì´
        def pick_fault_pattern(pats, cnts):
            idxs = [idx for idx, p in enumerate(pats) if (p == 0).any()]
            if not idxs:
                return None, None
            # ê³ ì¥ íŒ¨í„´ ì¤‘ì—ì„œ ì¹´ìš´íŠ¸ê°€ ê°€ì¥ í° ê²ƒì„ ëŒ€í‘œë¡œ ì„ íƒ
            best = max(idxs, key=lambda j: cnts[j])
            return pats[best], cnts[best]

        gt_fault_pat, _ = pick_fault_pattern(gt_pats, gt_cnts)
        pr_fault_pat, _ = pick_fault_pattern(pr_pats, pr_cnts)

        if gt_fault_pat is not None and pr_fault_pat is not None:
            t_true = first_index_close(gt, gt_fault_pat, tol=HAMMING_TOL)
            t_pred = first_index_close(pr, pr_fault_pat, tol=HAMMING_TOL)
            if (t_true is not None) and (t_pred is not None):
                maj_delays.append(max(t_pred - t_true, 0) * dt)

print("\n==== [Majority Top-2] pattern-match accuracy ====")
print(f"Majority top-2 acc: {maj_ok}/{N} = {maj_ok/max(N,1):.4f}")
if maj_delays:
    d = np.array(maj_delays)
    print(f"Majority delay (success only) â†’ mean={d.mean():.4f}s, median={np.median(d):.4f}s, n={len(maj_delays)}")
else:
    print("Majority delay: (no majority successes)")
